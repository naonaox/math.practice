{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winery classification with the bivariate Gaussian\n",
    "\n",
    "Our first generative model for Winery classification used just one feature. Now we use two features, modeling each class by a **bivariate Gaussian**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the univariate case, we start by loading in the Wine data set. Make sure the file `wine.data.txt` is in the same directory as this notebook.\n",
    "\n",
    "Recall that there are 178 data points, each with 13 features and a label (1,2,3). As before, we will divide this into a training set of 130 points and a test set of 48 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Useful module for dealing with the Gaussian density\n",
    "from scipy.stats import norm, multivariate_normal \n",
    "# installing packages for interactive graphs\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set.\n",
    "data = np.loadtxt('wine.data.txt', delimiter=',')\n",
    "# Names of features\n",
    "featurenames = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash','Magnesium', 'Total phenols', \n",
    "                'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n",
    "                'OD280/OD315 of diluted wines', 'Proline']\n",
    "# Split 178 instances into training set (trainx, trainy) of size 130 and test set (testx, testy) of size 48\n",
    "np.random.seed(0)\n",
    "perm = np.random.permutation(178)\n",
    "trainx = data[perm[0:130],1:14]\n",
    "trainy = data[perm[0:130],0]\n",
    "testx = data[perm[130:178], 1:14]\n",
    "testy = data[perm[130:178],0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Look at the distribution of two features from one of the wineries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to plot the distribution of two features from a particular winery. We will use several helper functions for this. It is worth understanding each of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first helper function fits a Gaussian to a data set, restricting attention to specified features.\n",
    "It returns the mean and covariance matrix of the Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Gaussian to a data set using the selected features\n",
    "def fit_gaussian(x, features):\n",
    "    mu = np.mean(x[:,features], axis=0)\n",
    "    covar = np.cov(x[:,features], rowvar=0, bias=1)\n",
    "    return mu, covar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's look at the Gaussian we get for winery 1, using features 0 ('alcohol') and 6 ('flavanoids')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "[13.78534884  2.99627907]\n",
      "Covariance matrix:\n",
      "[[0.23325279 0.07526874]\n",
      " [0.07526874 0.15240941]]\n"
     ]
    }
   ],
   "source": [
    "f1 = 0\n",
    "f2 = 6\n",
    "label = 1\n",
    "mu, covar = fit_gaussian(trainx[trainy==label,:], [f1,f2])\n",
    "print(\"Mean:\\n\" + str(mu))\n",
    "print(\"Covariance matrix:\\n\" + str(covar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will construct a routine for displaying points sampled from a two-dimensional Gaussian, as well as a few contour lines. Part of doing this involves deciding what range to use for each axis. We begin with a little helper function that takes as input an array of numbers (values along a single feature) and returns the range in which these numbers lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the range within which an array of numbers lie, with a little buffer\n",
    "def find_range(x):\n",
    "    lower = min(x)\n",
    "    upper = max(x)\n",
    "    width = upper - lower\n",
    "    lower = lower - 0.2 * width\n",
    "    upper = upper + 0.2 * width\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a routine that plots a few contour lines of a given two-dimensional Gaussian.\n",
    "It takes as input:\n",
    "* `mu`, `cov`: the parameters of the Gaussian\n",
    "* `x1g`, `x2g`: the grid (along the two axes) at which the density is to be computed\n",
    "* `col`: the color of the contour lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(mu, cov, x1g, x2g, col):\n",
    "    rv = multivariate_normal(mean=mu, cov=cov)\n",
    "    z = np.zeros((len(x1g),len(x2g)))\n",
    "    for i in range(0,len(x1g)):\n",
    "        for j in range(0,len(x2g)):\n",
    "            z[j,i] = rv.logpdf([x1g[i], x2g[j]]) \n",
    "    sign, logdet = np.linalg.slogdet(cov)\n",
    "    normalizer = -0.5 * (2 * np.log(6.28) + sign * logdet)\n",
    "    for offset in range(1,4):\n",
    "        plt.contour(x1g,x2g,z, levels=[normalizer - offset], colors=col, linewidths=2.0, linestyles='solid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **two_features_plot** takes an input two features and a label, and displays the distribution for the specified winery and pair of features.\n",
    "\n",
    "The first line allows you to specify the parameters interactively using sliders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d249dfd20d46639f11d10a2fa13332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=12), IntSlider(value=6, description='f2', max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual( f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1), label=IntSlider(1,1,3,1) )\n",
    "def two_features_plot(f1,f2,label):\n",
    "    if f1 == f2: # we need f1 != f2\n",
    "        print(\"Please choose different features for f1 and f2.\")\n",
    "        return  \n",
    "    \n",
    "    # Set up plot\n",
    "    x1_lower, x1_upper = find_range(trainx[trainy==label,f1])\n",
    "    x2_lower, x2_upper = find_range(trainx[trainy==label,f2])\n",
    "    plt.xlim(x1_lower, x1_upper) # limit along x1-axis\n",
    "    plt.ylim(x2_lower, x2_upper) # limit along x2-axis\n",
    "    \n",
    "    # Plot the training points along the two selected features\n",
    "    plt.plot(trainx[trainy==label, f1], trainx[trainy==label, f2], 'ro')\n",
    "\n",
    "    # Define a grid along each axis; the density will be computed at each grid point\n",
    "    res = 200 # resolution\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "\n",
    "    # Now plot a few contour lines of the density\n",
    "    mu, cov = fit_gaussian(trainx[trainy==label,:], [f1,f2])\n",
    "    plot_contours(mu, cov, x1g, x2g, 'k')\n",
    "    \n",
    "    # Finally, display\n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.title('Class ' + str(label), fontsize=14, color='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fit a Gaussian to each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function that will fit a Gaussian generative model to the three classes, restricted to a given list of features. The function returns:\n",
    "* `mu`: the means of the Gaussians, one per row\n",
    "* `covar`: covariance matrices of each of the Gaussians\n",
    "* `pi`: list of three class weights summing to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes y takes on values 1,2,3\n",
    "def fit_generative_model(x, y, features):\n",
    "    k = 3 # number of classes\n",
    "    d = len(features) # number of features\n",
    "    mu = np.zeros((k+1,d)) # list of means\n",
    "    covar = np.zeros((k+1,d,d)) # list of covariance matrices\n",
    "    pi = np.zeros(k+1) # list of class weights\n",
    "    for label in range(1,k+1):\n",
    "        indices = (y==label)\n",
    "        mu[label,:], covar[label,:,:] = fit_gaussian(x[indices,:], features)\n",
    "        pi[label] = float(sum(indices))/float(len(y))\n",
    "    return mu, covar, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the three Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3869f24380c4339a3cbf5d3df840fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=12), IntSlider(value=6, description='f2', max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual( f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1) )\n",
    "def three_class_plot(f1,f2):\n",
    "    if f1 == f2: # we need f1 != f2\n",
    "        print(\"Please choose different features for f1 and f2.\")\n",
    "        return  \n",
    "    \n",
    "    # Set up plot\n",
    "    x1_lower, x1_upper = find_range(trainx[:,f1])\n",
    "    x2_lower, x2_upper = find_range(trainx[:,f2])\n",
    "    plt.xlim(x1_lower, x1_upper) # limit along x1-axis\n",
    "    plt.ylim(x2_lower, x2_upper) # limit along x2-axis\n",
    "    \n",
    "    # Plot the training points along the two selected features\n",
    "    colors = ['r', 'k', 'g']\n",
    "    for label in range(1,4):\n",
    "        plt.plot(trainx[trainy==label,f1], trainx[trainy==label,f2], marker='o', ls='None', c=colors[label-1])\n",
    "\n",
    "    # Define a grid along each axis; the density will be computed at each grid point\n",
    "    res = 200 # resolution\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "\n",
    "    # Show the Gaussian fit to each class, using features f1,f2\n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, [f1,f2])\n",
    "    for label in range(1,4):\n",
    "        gmean = mu[label,:]\n",
    "        gcov = covar[label,:,:]\n",
    "        plot_contours(gmean, gcov, x1g, x2g, colors[label-1])\n",
    "\n",
    "    # Finally, display\n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.title('Wine data', fontsize=14, color='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict labels for the test points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well we can predict the class (1,2,3) based just on these two features?\n",
    "\n",
    "We start with a testing procedure that is analogous to what we developed in the 1-d case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850496748d77458993c34bdc4f219efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=12), IntSlider(value=6, description='f2', max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now test the performance of a predictor based on a subset of features\n",
    "@interact( f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1) )\n",
    "def test_model(f1, f2):\n",
    "    if f1 == f2: # need f1 != f2\n",
    "        print(\"Please choose different features for f1 and f2.\")\n",
    "        return  \n",
    "    features= [f1,f2]\n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, features)\n",
    "    \n",
    "    k = 3 # Labels 1,2,...,k\n",
    "    nt = len(testy) # Number of test points\n",
    "    score = np.zeros((nt,k+1))\n",
    "    for i in range(0,nt):\n",
    "        for label in range(1,k+1):\n",
    "            score[i,label] = np.log(pi[label]) + \\\n",
    "            multivariate_normal.logpdf(testx[i,features], mean=mu[label,:], cov=covar[label,:,:])\n",
    "    predictions = np.argmax(score[:,1:4], axis=1) + 1\n",
    "    # Finally, tally up score\n",
    "    errors = np.sum(predictions != testy)\n",
    "    print( \"Test error using feature(s): \",)\n",
    "    for f in features:\n",
    "        print( \"'\" + featurenames[f] + \"'\" + \" \",)\n",
    "    print()\n",
    "    print( \"Errors: \" + str(errors) + \"/\" + str(nt))# Now test the performance of a predictor based on a subset of features\n",
    "    return errors,nt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Fast exercise 1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different pairs of features yield different test errors.\n",
    "* What is the smallest achievable test error?\n",
    "* Which pair of features achieves this minimum test error?\n",
    "\n",
    "*Make a note of your answers to these questions, as you will need to enter them as part of this week's assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Malic acid' \n",
      "\n",
      "Errors: 9/48\n",
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Ash' \n",
      "\n",
      "Errors: 12/48\n",
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Alcalinity of ash' \n",
      "\n",
      "Errors: 9/48\n",
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Magnesium' \n",
      "\n",
      "Errors: 14/48\n",
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Total phenols' \n",
      "\n",
      "Errors: 5/48\n",
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Flavanoids' \n",
      "\n",
      "Errors: 4/48\n",
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Nonflavanoid phenols' \n",
      "\n",
      "Errors: 10/48\n",
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Proanthocyanins' \n",
      "\n",
      "Errors: 8/48\n",
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Color intensity' \n",
      "\n",
      "Errors: 8/48\n",
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Hue' \n",
      "\n",
      "Errors: 6/48\n",
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'OD280/OD315 of diluted wines' \n",
      "\n",
      "Errors: 5/48\n",
      "Test error using feature(s): \n",
      "'Alcohol' \n",
      "'Proline' \n",
      "\n",
      "Errors: 9/48\n",
      "Test error using feature(s): \n",
      "'Malic acid' \n",
      "'Ash' \n",
      "\n",
      "Errors: 22/48\n",
      "Test error using feature(s): \n",
      "'Malic acid' \n",
      "'Alcalinity of ash' \n",
      "\n",
      "Errors: 16/48\n",
      "Test error using feature(s): \n",
      "'Malic acid' \n",
      "'Magnesium' \n",
      "\n",
      "Errors: 11/48\n",
      "Test error using feature(s): \n",
      "'Malic acid' \n",
      "'Total phenols' \n",
      "\n",
      "Errors: 12/48\n",
      "Test error using feature(s): \n",
      "'Malic acid' \n",
      "'Flavanoids' \n",
      "\n",
      "Errors: 9/48\n",
      "Test error using feature(s): \n",
      "'Malic acid' \n",
      "'Nonflavanoid phenols' \n",
      "\n",
      "Errors: 21/48\n",
      "Test error using feature(s): \n",
      "'Malic acid' \n",
      "'Proanthocyanins' \n",
      "\n",
      "Errors: 14/48\n",
      "Test error using feature(s): \n",
      "'Malic acid' \n",
      "'Color intensity' \n",
      "\n",
      "Errors: 11/48\n",
      "Test error using feature(s): \n",
      "'Malic acid' \n",
      "'Hue' \n",
      "\n",
      "Errors: 16/48\n",
      "Test error using feature(s): \n",
      "'Malic acid' \n",
      "'OD280/OD315 of diluted wines' \n",
      "\n",
      "Errors: 15/48\n",
      "Test error using feature(s): \n",
      "'Malic acid' \n",
      "'Proline' \n",
      "\n",
      "Errors: 9/48\n",
      "Test error using feature(s): \n",
      "'Ash' \n",
      "'Alcalinity of ash' \n",
      "\n",
      "Errors: 18/48\n",
      "Test error using feature(s): \n",
      "'Ash' \n",
      "'Magnesium' \n",
      "\n",
      "Errors: 23/48\n",
      "Test error using feature(s): \n",
      "'Ash' \n",
      "'Total phenols' \n",
      "\n",
      "Errors: 16/48\n",
      "Test error using feature(s): \n",
      "'Ash' \n",
      "'Flavanoids' \n",
      "\n",
      "Errors: 10/48\n",
      "Test error using feature(s): \n",
      "'Ash' \n",
      "'Nonflavanoid phenols' \n",
      "\n",
      "Errors: 22/48\n",
      "Test error using feature(s): \n",
      "'Ash' \n",
      "'Proanthocyanins' \n",
      "\n",
      "Errors: 21/48\n",
      "Test error using feature(s): \n",
      "'Ash' \n",
      "'Color intensity' \n",
      "\n",
      "Errors: 13/48\n",
      "Test error using feature(s): \n",
      "'Ash' \n",
      "'Hue' \n",
      "\n",
      "Errors: 13/48\n",
      "Test error using feature(s): \n",
      "'Ash' \n",
      "'OD280/OD315 of diluted wines' \n",
      "\n",
      "Errors: 13/48\n",
      "Test error using feature(s): \n",
      "'Ash' \n",
      "'Proline' \n",
      "\n",
      "Errors: 15/48\n",
      "Test error using feature(s): \n",
      "'Alcalinity of ash' \n",
      "'Magnesium' \n",
      "\n",
      "Errors: 11/48\n",
      "Test error using feature(s): \n",
      "'Alcalinity of ash' \n",
      "'Total phenols' \n",
      "\n",
      "Errors: 12/48\n",
      "Test error using feature(s): \n",
      "'Alcalinity of ash' \n",
      "'Flavanoids' \n",
      "\n",
      "Errors: 8/48\n",
      "Test error using feature(s): \n",
      "'Alcalinity of ash' \n",
      "'Nonflavanoid phenols' \n",
      "\n",
      "Errors: 19/48\n",
      "Test error using feature(s): \n",
      "'Alcalinity of ash' \n",
      "'Proanthocyanins' \n",
      "\n",
      "Errors: 20/48\n",
      "Test error using feature(s): \n",
      "'Alcalinity of ash' \n",
      "'Color intensity' \n",
      "\n",
      "Errors: 11/48\n",
      "Test error using feature(s): \n",
      "'Alcalinity of ash' \n",
      "'Hue' \n",
      "\n",
      "Errors: 7/48\n",
      "Test error using feature(s): \n",
      "'Alcalinity of ash' \n",
      "'OD280/OD315 of diluted wines' \n",
      "\n",
      "Errors: 9/48\n",
      "Test error using feature(s): \n",
      "'Alcalinity of ash' \n",
      "'Proline' \n",
      "\n",
      "Errors: 13/48\n",
      "Test error using feature(s): \n",
      "'Magnesium' \n",
      "'Total phenols' \n",
      "\n",
      "Errors: 11/48\n",
      "Test error using feature(s): \n",
      "'Magnesium' \n",
      "'Flavanoids' \n",
      "\n",
      "Errors: 7/48\n",
      "Test error using feature(s): \n",
      "'Magnesium' \n",
      "'Nonflavanoid phenols' \n",
      "\n",
      "Errors: 18/48\n",
      "Test error using feature(s): \n",
      "'Magnesium' \n",
      "'Proanthocyanins' \n",
      "\n",
      "Errors: 9/48\n",
      "Test error using feature(s): \n",
      "'Magnesium' \n",
      "'Color intensity' \n",
      "\n",
      "Errors: 10/48\n",
      "Test error using feature(s): \n",
      "'Magnesium' \n",
      "'Hue' \n",
      "\n",
      "Errors: 7/48\n",
      "Test error using feature(s): \n",
      "'Magnesium' \n",
      "'OD280/OD315 of diluted wines' \n",
      "\n",
      "Errors: 7/48\n",
      "Test error using feature(s): \n",
      "'Magnesium' \n",
      "'Proline' \n",
      "\n",
      "Errors: 14/48\n",
      "Test error using feature(s): \n",
      "'Total phenols' \n",
      "'Flavanoids' \n",
      "\n",
      "Errors: 7/48\n",
      "Test error using feature(s): \n",
      "'Total phenols' \n",
      "'Nonflavanoid phenols' \n",
      "\n",
      "Errors: 13/48\n",
      "Test error using feature(s): \n",
      "'Total phenols' \n",
      "'Proanthocyanins' \n",
      "\n",
      "Errors: 15/48\n",
      "Test error using feature(s): \n",
      "'Total phenols' \n",
      "'Color intensity' \n",
      "\n",
      "Errors: 5/48\n",
      "Test error using feature(s): \n",
      "'Total phenols' \n",
      "'Hue' \n",
      "\n",
      "Errors: 8/48\n",
      "Test error using feature(s): \n",
      "'Total phenols' \n",
      "'OD280/OD315 of diluted wines' \n",
      "\n",
      "Errors: 10/48\n",
      "Test error using feature(s): \n",
      "'Total phenols' \n",
      "'Proline' \n",
      "\n",
      "Errors: 9/48\n",
      "Test error using feature(s): \n",
      "'Flavanoids' \n",
      "'Nonflavanoid phenols' \n",
      "\n",
      "Errors: 8/48\n",
      "Test error using feature(s): \n",
      "'Flavanoids' \n",
      "'Proanthocyanins' \n",
      "\n",
      "Errors: 8/48\n",
      "Test error using feature(s): \n",
      "'Flavanoids' \n",
      "'Color intensity' \n",
      "\n",
      "Errors: 3/48\n",
      "Test error using feature(s): \n",
      "'Flavanoids' \n",
      "'Hue' \n",
      "\n",
      "Errors: 4/48\n",
      "Test error using feature(s): \n",
      "'Flavanoids' \n",
      "'OD280/OD315 of diluted wines' \n",
      "\n",
      "Errors: 7/48\n",
      "Test error using feature(s): \n",
      "'Flavanoids' \n",
      "'Proline' \n",
      "\n",
      "Errors: 4/48\n",
      "Test error using feature(s): \n",
      "'Nonflavanoid phenols' \n",
      "'Proanthocyanins' \n",
      "\n",
      "Errors: 18/48\n",
      "Test error using feature(s): \n",
      "'Nonflavanoid phenols' \n",
      "'Color intensity' \n",
      "\n",
      "Errors: 12/48\n",
      "Test error using feature(s): \n",
      "'Nonflavanoid phenols' \n",
      "'Hue' \n",
      "\n",
      "Errors: 14/48\n",
      "Test error using feature(s): \n",
      "'Nonflavanoid phenols' \n",
      "'OD280/OD315 of diluted wines' \n",
      "\n",
      "Errors: 13/48\n",
      "Test error using feature(s): \n",
      "'Nonflavanoid phenols' \n",
      "'Proline' \n",
      "\n",
      "Errors: 11/48\n",
      "Test error using feature(s): \n",
      "'Proanthocyanins' \n",
      "'Color intensity' \n",
      "\n",
      "Errors: 5/48\n",
      "Test error using feature(s): \n",
      "'Proanthocyanins' \n",
      "'Hue' \n",
      "\n",
      "Errors: 11/48\n",
      "Test error using feature(s): \n",
      "'Proanthocyanins' \n",
      "'OD280/OD315 of diluted wines' \n",
      "\n",
      "Errors: 16/48\n",
      "Test error using feature(s): \n",
      "'Proanthocyanins' \n",
      "'Proline' \n",
      "\n",
      "Errors: 10/48\n",
      "Test error using feature(s): \n",
      "'Color intensity' \n",
      "'Hue' \n",
      "\n",
      "Errors: 8/48\n",
      "Test error using feature(s): \n",
      "'Color intensity' \n",
      "'OD280/OD315 of diluted wines' \n",
      "\n",
      "Errors: 9/48\n",
      "Test error using feature(s): \n",
      "'Color intensity' \n",
      "'Proline' \n",
      "\n",
      "Errors: 5/48\n",
      "Test error using feature(s): \n",
      "'Hue' \n",
      "'OD280/OD315 of diluted wines' \n",
      "\n",
      "Errors: 11/48\n",
      "Test error using feature(s): \n",
      "'Hue' \n",
      "'Proline' \n",
      "\n",
      "Errors: 7/48\n",
      "Test error using feature(s): \n",
      "'OD280/OD315 of diluted wines' \n",
      "'Proline' \n",
      "\n",
      "Errors: 4/48\n",
      "[((6, 9), 0.0625), ((0, 6), 0.08333333333333333), ((6, 10), 0.08333333333333333), ((6, 12), 0.08333333333333333), ((11, 12), 0.08333333333333333), ((0, 5), 0.10416666666666667), ((0, 11), 0.10416666666666667), ((5, 9), 0.10416666666666667), ((8, 9), 0.10416666666666667), ((9, 12), 0.10416666666666667), ((0, 10), 0.125), ((3, 10), 0.14583333333333334), ((4, 6), 0.14583333333333334), ((4, 10), 0.14583333333333334), ((4, 11), 0.14583333333333334), ((5, 6), 0.14583333333333334), ((6, 11), 0.14583333333333334), ((10, 12), 0.14583333333333334), ((0, 8), 0.16666666666666666), ((0, 9), 0.16666666666666666), ((3, 6), 0.16666666666666666), ((5, 10), 0.16666666666666666), ((6, 7), 0.16666666666666666), ((6, 8), 0.16666666666666666), ((9, 10), 0.16666666666666666), ((0, 1), 0.1875), ((0, 3), 0.1875), ((0, 12), 0.1875), ((1, 6), 0.1875), ((1, 12), 0.1875), ((3, 11), 0.1875), ((4, 8), 0.1875), ((5, 12), 0.1875), ((9, 11), 0.1875), ((0, 7), 0.20833333333333334), ((2, 6), 0.20833333333333334), ((4, 9), 0.20833333333333334), ((5, 11), 0.20833333333333334), ((8, 12), 0.20833333333333334), ((1, 4), 0.22916666666666666), ((1, 9), 0.22916666666666666), ((3, 4), 0.22916666666666666), ((3, 9), 0.22916666666666666), ((4, 5), 0.22916666666666666), ((7, 12), 0.22916666666666666), ((8, 10), 0.22916666666666666), ((10, 11), 0.22916666666666666), ((0, 2), 0.25), ((1, 5), 0.25), ((3, 5), 0.25), ((7, 9), 0.25), ((2, 9), 0.2708333333333333), ((2, 10), 0.2708333333333333), ((2, 11), 0.2708333333333333), ((3, 12), 0.2708333333333333), ((5, 7), 0.2708333333333333), ((7, 11), 0.2708333333333333), ((0, 4), 0.2916666666666667), ((1, 8), 0.2916666666666667), ((4, 12), 0.2916666666666667), ((7, 10), 0.2916666666666667), ((1, 11), 0.3125), ((2, 12), 0.3125), ((5, 8), 0.3125), ((1, 3), 0.3333333333333333), ((1, 10), 0.3333333333333333), ((2, 5), 0.3333333333333333), ((8, 11), 0.3333333333333333), ((2, 3), 0.375), ((4, 7), 0.375), ((7, 8), 0.375), ((3, 7), 0.3958333333333333), ((3, 8), 0.4166666666666667), ((1, 7), 0.4375), ((2, 8), 0.4375), ((1, 2), 0.4583333333333333), ((2, 7), 0.4583333333333333), ((2, 4), 0.4791666666666667)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "pairfeatures=list(combinations(range(13),2))\n",
    "result=[test_model(x[0],x[1]) for x in pairfeatures]\n",
    "error_rate=[x[0]*1.0/x[1] for x in result]\n",
    "outlist=sorted(zip(pairfeatures,error_rate),key=lambda x:x[1])\n",
    "print(outlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. The decision boundary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **show_decision_boundary** takes as input two features, builds a classifier based only on these two features, and shows a plot that contains both the training data and the decision boundary.\n",
    "\n",
    "To compute the decision boundary, a dense grid is defined on the two-dimensional input space and the classifier is applied to every grid point. The built-in `pyplot.contour` function can then be invoked to depict the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_decision_boundary(f1,f2):\n",
    "    # Fit Gaussian to each class\n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, [f1,f2])\n",
    "    \n",
    "    # Set up dimensions of plot\n",
    "    x1_lower, x1_upper = find_range(trainx[:,f1])\n",
    "    x2_lower, x2_upper = find_range(trainx[:,f2])\n",
    "    plt.xlim([x1_lower,x1_upper])\n",
    "    plt.ylim([x2_lower,x2_upper])\n",
    "\n",
    "    # Plot points in training set\n",
    "    colors = ['r', 'k', 'g']\n",
    "    for label in range(1,4):\n",
    "        plt.plot(trainx[trainy==label,f1], trainx[trainy==label,f2], marker='o', ls='None', c=colors[label-1])\n",
    "\n",
    "    # Define a dense grid; every point in the grid will be classified according to the generative model\n",
    "    res = 200\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "\n",
    "    # Declare random variables corresponding to each class density\n",
    "    random_vars = {}\n",
    "    for label in range(1,4):\n",
    "        random_vars[label] = multivariate_normal(mean=mu[label,:],cov=covar[label,:,:])\n",
    "\n",
    "    # Classify every point in the grid; these are stored in an array Z[]\n",
    "    Z = np.zeros((len(x1g), len(x2g)))\n",
    "    for i in range(0,len(x1g)):\n",
    "        for j in range(0,len(x2g)):\n",
    "            scores = []\n",
    "            for label in range(1,4):\n",
    "                scores.append(np.log(pi[label]) + random_vars[label].logpdf([x1g[i],x2g[j]]))\n",
    "            Z[i,j] = np.argmax(scores) + 1\n",
    "\n",
    "    # Plot the contour lines\n",
    "    plt.contour(x1g,x2g,Z.T,3,cmap='seismic')\n",
    "    \n",
    "    # Finally, show the image\n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the function above to draw the decision boundary using features 0 ('alcohol') and 6 ('flavanoids')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_decision_boundary(0,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Fast exercise 2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you add interactive sliders to function **show_decision_boundary**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Fast exercise 3</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a plot similar to that of **show_decision_boundary**, but in which just the **test** data is shown.\n",
    "Look back at your answer to *Fast exercise 1*. Is it corroborated by your plot? Are the errors clearly visible?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
